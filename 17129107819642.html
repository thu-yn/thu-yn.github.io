<!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
    CT3D: Improving 3D Object Detection with Channel-wise Transformer - Prepare for the FUTURE
    
    </title>
    

    
    
    <link href="atom.xml" rel="alternate" title="Prepare for the FUTURE" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/style.min.css">
    <link rel="stylesheet" href="asset/css/doc.css">
    <script src="asset/app.js"></script>
</head>
  <body>
    <section class="hero">
      <div class="hero-head">
          <nav class="navbar" role="navigation" aria-label="main navigation">
              <div class="container">
              <div class="navbar-brand">
                
                <a target="_self" class="navbar-item " href="index.html">Home</a>
                
                <a target="_self" class="navbar-item " href="archives.html">Archives</a>
                

                <a role="button" id="navbarSNSRssSwitchBtn" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navbarSNSRssButtons">
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                </a>
              </div>
            
              <div id="navbarSNSRssButtons" class="navbar-menu">
                <div class="navbar-start">
                  
                </div>
            
                <div class="navbar-end">
                  <div class="navbar-item">
                    <!--buttons start-->
                    <div class="buttons">
                      
                        
                        
                        
                        
                      
                      <a href="atom.xml" target="_blank" title="RSS">
                          <span class="icon is-large has-text-black-bis">
                              <svg class="svg-inline--fa fa-rss fa-w-14 fa-lg" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg><!-- <i class="fas fa-rss fa-lg"></i> -->
                          </span>
                      </a>
                    </div>
                    <!--buttons end-->

                  </div>
                </div>
                </div>
              </div>
            </nav>
      </div>

 <div class="hero-body ct-body"></div>
      
    </section>
    <section class="ct-body">
      <div class="container">
          <div class="columns is-variable bd-klmn-columns is-4 is-centered">
              <div class="column is-four-fifths">
                  <div class="post-body single-content">
                    
                    <h1 class="title">
                            CT3D: Improving 3D Object Detection with Channel-wise Transformer   
                      </h1>
                     
                    
                      <div class="media">
                            
                            <div class="media-content">
                              <div class="content">
                                <p>
                                 <span class="date">2024/04/12</span>
                                  
                                         
                                  

                                   
                                      
                                  <br />
                                  <span class="tran-tags">Tags:</span>&nbsp;
                                  
                                    <a class="tag is-link is-light" href='tag_Pointcloud%20in%20Transformer.html'>#Pointcloud in Transformer</a>
                                     

                                </p>
                              </div>
                            </div>
                         
                    </div>
                </div>
                  <article class="markdown-body single-content">
                    <blockquote>
<p>选择它的原因：</p>
<ol>
<li>利用 Transformer 对 3D 点云进行目标检测，与动点剔除任务较为接近；</li>
<li>采用 \(KITTI\) 室外数据集进行评测，与我目前使用的 \( SemanticKitti\) 数据集同源；</li>
<li><a href="https://github.com/hlsheng1/CT3D" title="github: CT3D">代码</a>在 github 开源，易于学习。</li>
<li>代码是基于 <a href="https://blog.csdn.net/zyw2002/article/details/127419245" title="OpenPCDet">OpenPCDet</a> 框架，数据 - 模型分离。</li>
</ol>
</blockquote>
<h2><a id="1-advantages" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Advantages</h2>
<ol>
<li>CT3D 是一种新颖的端到端两阶段 3D 物体检测框架，方便集成于各个 <a href="https://zhuanlan.zhihu.com/p/138515680" title="RPN">RPN</a> backbone；</li>
<li>引入了 proposal-to-point 嵌入来在编码器模块中有效地编码 RPN 提案信息。</li>
<li>利用通道重新加权方法来增强 Standard Transformer Decoder。</li>
</ol>
<h2><a id="2-pipeline" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Pipeline</h2>
<h3><a id="2-1-overview" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1 Overview</h3>
<div style="text-align: center">
<img src="media/17129107819642/3.png"/>
<p>图 1 CT3D Pipeline</p>
</div>
<p>原始点首先被输入到 RPN 中以生成 3D proposal。 然后，原始点以及相应的 proposal 由 proposal-to-point 编码模块和 channel-wise 解码模块组成的通道 Transformer 进行处理。 具体来说，proposal-to-point 编码模块是用全局 proposal 感知上下文信息来调制每个点特征。 之后，编码后的点特征通过 channel-wise 解码模块转换为有效的 proposal 特征表示，用于置信度预测和框回归。</p>
<h3><a id="2-2-rpn-module" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2 RPN Module</h3>
<h4><a id="2-2-1-goal" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2.1 Goal</h4>
<p>对一个拥有 \(C\) 维特征和三维坐标的点云 \(\mathbf{P}\)，经过 RPN 网络后，生成 3D 边界框。其中边界框包括：中心坐标 \(\mathbf{p}^c = [x^c, y^c, z^c]\)，长度 \(l^c\), 宽度 \(w^c\)，高度 \(h^c\) 以及方向 \(\theta^c\)。</p>
<p>在本文中，采用 <a href="https://www.mdpi.com/1424-8220/18/10/3337?ref=https://codemonkey.link" title="SECOND">SECOND</a> 的 3D voxel CNN 作为 default <a href="https://thu-yn.github.io/17129124969573.html" title="RPN">RPN</a>。</p>
<h3><a id="2-3-proposal-to-point-encoding-module" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3 Proposal-to-point Encoding Module</h3>
<h4><a id="2-3-1-goal" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.1 Goal</h4>
<p>为了增强 RPN 提出的 proposals，采用两步战略：首先采用 proposal-to-point embedding，将 proposal 映射为点特征；其次采用自注意力编码，通过对相应 proposal 内点之间的相对关系进行建模来细化点特征。</p>
<div style="text-align: center">
<img src="media/17129107819642/13.png"/>
<p>图 2 Proposal-to-point encoding Pipeline</p>
</div>
<h4><a id="2-3-2-proposal-to-point-embedding" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.2 Proposal-to-point Embedding</h4>
<p>得到 RPN 生成的 Proposal 后，根据 proposal 在点云中划分出按比例缩放的 RoI 区域，目的是希望通过尽可能多的对象点来补偿 proposal 和相应的地面实况框之间的误差。</p>
<p>具体的，缩放后的 RoI 区域是一个无限高度，半径为 \( r=\alpha \sqrt{(\frac{l^c}{2})^2+(\frac{w^c}{2})^2}\) 的圆柱体，其中 \(\alpha\) 是超参数，\( l^c、w^c\) 分别表示 proposal 的长度和宽度。接着从经过缩放的 RoI 区域中随机采样 \(N=256\) 个点用作后续处理。\((N=\{ \mathbf{p}_1,...\mathbf{p}_N \})\)</p>
<p>首先计算每个采样点和 proposal 中心点之间的相对坐标，以统一输入距离特征，表示为 \(\Delta \mathbf{p}^c_i=\mathbf{p}_i-\mathbf{p}^c, \forall \mathbf{p}_i \in \mathcal{N}\)。接着可以将 proposal 信息与每个点特征相结合，如第 \(i\) 个点的特征可以表示为：\([\Delta \mathbf{p}^c_i,l^c,w^c,h^c,\theta ^c,f^r_i]\)，这里 \(f^r_i\) 表示诸如反射率等原始点云特征。</p>
<p>本文提出一种新颖的关键点减法策略计算每个点和其所在 proposal 的八个角点的差。计算相对坐标的方式为：\(\Delta \mathbf{p}^j_i=\mathbf{p}_i-\mathbf{p}^j, j=1,...,8\)，这里 \(\mathbf{p}^j\) 表示第 \(j\) 个角点的坐标。此时前文提到的一些特征信息（例如 \(l^c,w^c,h^c 以及 \theta ^c\)）可以体现在不同维度的距离信息中。通过这种方式，新生成的相对坐标 \(\Delta \mathbf{p}^j_i\) 可以视为对 proposal 信息更好的表示。对每个点云 \(\mathbf{p}_i\)，proposal-guided 特征可以表示为：</p>
<p>\( \begin{align}  \mathbf{f}_i = \mathcal{A} ([\Delta \mathbf{p}^c_i, \Delta \mathbf{p}^1_i, ..., \Delta \mathbf{p}^8_i, f^r_i])\in \mathbb{R}^D \end{align}\)</p>
<p>这里 \(\mathcal{A}(·)\) 是线性投影层，将点云特征投影到一个高维度的 Embedding 层中。</p>
<h4><a id="2-3-3-self-attention-encoding" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.3.3 Self-attention Encoding</h4>
<p>经过 Proposal-to-point Encoding Module 后的点云特征随后被送入多头自注意力层中，接着是具有残差结构的 FCN 中，用来编码丰富的上下文关系和 proposal 中点的依赖性，以细化点特征。</p>
<p>多头自注意力层与标准 Transformer 中相比，缺少了位置编码环节，原因是位置信息已经包含在点云特征中。定义 \(\mathbf{X}=[\mathbf{f}^T_1,...\mathbf{f}^T_N]^T \in \mathbb{R}^{N \times D}\) 作为具有 D 个维度的嵌入点特征。则有 \( \mathbf{Q}=\mathbf{W}_q \mathbf{X}; \mathbf{K}=\mathbf{W}_k \mathbf{X}; \mathbf{V}=\mathbf{W}_v \mathbf{X}\)，这里 \( \mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{N \times N}\) 是线性投影，\(\mathbf{Q}, \mathbf{K}, \mathbf{V}\) 称作 query，key 和 value 嵌入，这三个嵌入将作为多头自注意力的输入进行处理。在一个具有 \(H\) 个 head 的注意力模块中，\(\mathbf{Q}, \mathbf{K}, \mathbf{V}\) 可以进一步被分为 \(\mathbf{Q}=[\mathbf{Q}_1,...,\mathbf{Q}_H], \mathbf{K}=[\mathbf{K}_1,...,\mathbf{K}_H], \mathbf{V}=[\mathbf{V}_1,...,\mathbf{V}_H]\)，这里 \(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h \in \mathbb{R}^{N \times D'}, \forall h=1,...,H, D'=\frac{D}{H}\)。多头自注意力的输出可以表示为：</p>
<p>\( \begin{align}  S^{(att)}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = [\sigma (\frac {\mathbf{Q}_h \mathbf{K}^T_h} {\sqrt{D'}}) · \mathbf{V}_h], h=1,...,H \end{align}\)</p>
<p>这里 \(\sigma (·) \) 是 \(softmax\) 函数。接着通过一个简单的 FFN 层和残差算子，得到的结果如下：</p>
<p>\( \begin{align}  S^{(att)}(\mathbf{X}) = \mathcal{Z}(\mathcal{F}(\mathcal{Z}(S^{(att)}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) )))\end{align}\)</p>
<p>这里 \(\mathcal{Z}(·)\) 表示 add and norm，\(\mathcal{F}(·)\) 表示具有两个线性层和一个 \(ReLU\) 激活层的 FFN。最终选取 3 个 Self-attention Encoding 的框架堆叠起来得到 Proposal-to-point Encoding Module 的后半部分。</p>
<h3><a id="2-4-channel-wise-decoding-module" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4 Channel-wise Decoding Module</h3>
<h4><a id="2-4-1-goal" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4.1 Goal</h4>
<p>本模块计划对经过上一编码模块中得到的点特征 \(\hat{\mathbf{X}}\) 进行解码，以得到全局表示，并由 FFN 进一步处理，进行最终检测预测。</p>
<p>标准 Transformer 中 Decoder 使用交叉注意力机制对 \(M\) 个 Query Embedding 进行转换，而本文的 Decoder 仅根据以下两个事实对 1 个 Query Embedding 进行操作：</p>
<ol>
<li>\(M\) 个 Query Embedding 会产生较高的内存延迟，特别是在处理大量的 proposal 时；</li>
<li>\(M\) 个 Query Embedding 通常会独立的转化为 \(M\) 个单词或对象，而 proposal 的细化模型只需要一个预测。</li>
</ol>
<p>通常，解码器的输出的 proposal 表示可以被视为所有点特征的加权和，因此关键是确定专用于每个点的解码权重。</p>
<h4><a id="2-4-2-standard-decoding" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4.2 Standard Decoding</h4>
<p>标准解码方案利用 \(D\) 维可学习向量（Query Embedding）来聚合所有通道的点特征。每个注意力头中所有点特征的最终解码权重向量为：</p>
<p>\( \begin{align}  \mathcal{w}^{(S)}_h=\sigma (\frac{\hat {\mathbf {q}}_h \hat {\mathbf {K}}^T_h}{\sqrt {D'}}), \  h=1,...,H \end{align}\)</p>
<p>这里 \(\hat {\mathbf {K}}_h\) 是通过 Encoder 输出的投影计算出的第 \(h\) 个头的 key embedding，\(\hat {\mathbf {q}}_h\) 是与之相关的 query embedding。注意到 \(\hat {\mathbf {q}}_h \hat {\mathbf {K}}^T_h\) 向量的每个值可以视为每个单独点（或 key embedding）的全局聚合，并且后续的 \(softmax\) 函数根据归一化向量中的概率为每个点分配 Decoder 的值。</p>
<div style="text-align: center">
<img src="media/17129107819642/14.png"/>
<p>图 3 Standard Decoding Pipeline</p>
</div>
<p>因此，在标准 Transformer 中，Decoder 权重向量中的值是从简单的全局聚合导出的，缺乏局部通道建模。** 由于不同的通道通常在点云中表现出很强的几何关系，因此局部通道建模对于学习点云的 3D 表面结构至关重要。**</p>
<h4><a id="2-4-3-channel-wise-re-weighting" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4.3 Channel-wise Re-weighting</h4>
<p>为了强调 key embedding（即 \(\hat {\mathbf {K}}^T_h\)）中的通道信息，一个简单的解决方案是：基于 \(\hat {\mathbf {K}}^T_h\) 的所有通道计算点的 Decoder 权重向量，以获得 \(D\) 个 Decoder 值。并且对于这 \(D\) 个值，引入线性投影以生成统一的 channel-wise 解码向量。具体的 Decoder 中权重向量的 channel-wise 加权方式为：</p>
<p>\( \begin{align}  \mathcal{w}^{(C)}_h= \mathbf{s}· \hat{\sigma} (\frac{\hat {\mathbf {K}}^T_h}{\sqrt {D'}}), \  h=1,...,H \end{align}\)</p>
<p>其中 \(\mathbf{s}\) 是将 \(D'\) 个 Decoder 值压缩为 Re-weighting 的线性投影，\(\hat{\sigma} (·)\) 沿着 \(N\) 维度计算 \(softmax\)。</p>
<div style="text-align: center">
<img src="media/17129107819642/15.png"/>
<p>图 4 Channel-wise Re-weighting Pipeline</p>
</div>
<p>然而，由 \(\hat{\sigma} (·)\) 计算的解码权重忽略了每个点的全局聚合。因此，可以得到 ** 标准 Decoder 方案侧重于全局聚合，Channel-wise Re-weighting Decoder 方案侧重于通道方式局部聚合。** 基于以上两点，可以将其侧重点结合起来，得到一种 extended channel-wise re-weighting 的方案。</p>
<h4><a id="2-4-4-extended-channel-wise-re-weighting" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.4.4 Extended Channel-wise Re-weighting</h4>
<div style="text-align: center">
<img src="media/17129107819642/16.png"/>
<p>图 5 Entended Channel-wise Re-weighting Pipeline</p>
</div>
<p>首先重复 query embedding 和 key embedding 的矩阵乘积，将空间信息（全局聚合）传播到每个通道中；然后将输出以 element-wise 的形式与 key embedding 相乘以获取局部聚合，保持通道差异。其 decoder 的权重向量如下所示：</p>
<p>\( \begin{align}  \mathcal{w}^{(EC)}_h= \mathbf{s}· \hat{\sigma} (\frac{ \rho (\hat {\mathbf {q}}_h \hat {\mathbf {K}}^T_h)\odot  \hat {\mathbf {K}}^T_h}{\sqrt {D'}}), \  h=1,...,H \end{align}\)</p>
<p>其中 \(\rho (·)\) 是重复算子，使得 \(\hat {\mathbf {q}}_h \hat {\mathbf {K}}^T_h\) 的维度从 \(\mathbb{R} ^ {1 \times N} \to \mathbb{R} ^ {D' \times N}\)。此时，该权重向量不仅可以保持全局信息的捕捉，也可以丰富详细的局部通道交互。并且该方法仅带来 1K+(Bytes) 的增加。最终 Decoding 后的 proposal 可以描述如下：</p>
<p>\( \begin{align}  \mathcal {y} = [\mathcal {w}^{EC}_1 · \hat {V}_1, ... , \mathcal {w}^{EC}_H · \hat {V}_H ] \end{align}\)</p>
<p>这里 value embedding，也即 \(\hat {V}\)，是由 \(\hat {X}\) 经过线性投影得到。</p>
<h3><a id="2-5-detect-head-and-training-targets" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.5 Detect head and Training Targets</h3>
<p>经过先前步骤后，输入点特征被汇总为 \(D\) 维向量 \( \mathcal{y}\)，然后将其送入到两个 FFN 中，分别预测相对于输入 3D proposal 的置信度和框残差。</p>
<p>为了输出置信度，训练目标设置为 3D proposal 与其相应的地面实况框之间的 3D IoU。给定 3D proposal 的 IoU 以及相应的地面实况框，置信度预测目标如下所示：</p>
<p>\( \begin{align}  c^t = \min (1, \max (0, \frac {IoU - \alpha _B} {\alpha _F - \alpha _B})) \end{align}\)</p>
<p>这里 \(\alpha _F 和 \alpha _B\) 分别表示前景和后景的 IoU 阈值。</p>
<p>此外，回归目标（上标 \(t\)）由 proposal 和其相关的地面实况框（上标 \(g\)）得到：</p>
<p>\( \begin{align}  x^t=\frac {x^g-x^c} {d}, y^t=\frac {y^g-y^c} {d}, z^t=\frac {z^g-z^c} {h^c} \end{align}\)</p>
<p>\( \begin{align}  w^t=log(\frac {w^g} {w^c}), l^t=log(\frac {l^g} {l^c}), h^t=log(\frac {h^g} {h^c})  \end{align}\)</p>
<p>\( \begin{align}  \theta^t=\theta^g-\theta^c \end{align}\)</p>
<p>这里 \(d = \sqrt {(l^c)^2 + (w^c)^2}\) 是 proposal 底部的对角线。</p>
<h3><a id="2-6-training-losses" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.6 Training Losses</h3>
<p>采用端到端的策略来训练 CT3D。总的 training loss 是 RPN loss，置信度预测 loss 和框回归 loss 的总和，如下所示：</p>
<p>\( \begin{align}  \mathcal{L} = \mathcal{L}_{RPN} + \mathcal{L}_{conf} + \mathcal{L}_{reg} \end{align}\)</p>
<p>对预测置信度 \(c\) 采用二元交叉熵损失来计算置信度 loss：</p>
<p>\( \begin{align}  \mathcal{L}_{conf} = -c^t \log {c} - (1 - c^t) \log {(1 - c)} \end{align}\)</p>
<p>对于框回归 loss 计算：</p>
<p>\( \begin{align}  \mathcal{L}_{reg} = \mathbb{I} (IoU \ge \alpha _R) \sum _{\mu \in x,y,z,l,w,h,\theta} \mathcal{L}_{smooth-L1} (\mu, \mu ^t) \end{align}\)</p>
<p>这里 \(\mathbb{I} (IoU \ge \alpha _R)\) 表示只有 \(IoU \ge \alpha _R\) 的 proposal 才会被用来计算框回归 loss。</p>
<h2><a id="3-reproduction" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Reproduction</h2>
<h3><a id="3-1-system-environment" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1 System Environment</h3>
<p>Ubuntu 20.04.6 LTS</p>
<p>Conda 虚拟环境创建：在 <code>/home/workspace/nanyang</code> 的路径下激活 Conda 环境：</p>
<pre><code class="language-shell">source .bashrc      # conda init

conda create -n &quot;CT3D&quot; python=3.8   # 创建新 conda 环境

conda activate CT3D # 此时 conda 的环境是嵌套的，为(base(CT3D))

conda deactivate    # 此时是(base)

conda deactivate    # 此时没有 conda 环境

conda activate CT3D # 此时 conda 的环境不是嵌套的，为(CT3D)

# 安装 pytorch
pip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>Conda 环境基本信息：</p>
<table>
<thead>
<tr>
<th style="text-align: center">Name</th>
<th style="text-align: center">Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center">python</td>
<td style="text-align: center">3.8</td>
</tr>
<tr>
<td style="text-align: center">cuda</td>
<td style="text-align: center">11.0</td>
</tr>
<tr>
<td style="text-align: center">pytorch</td>
<td style="text-align: center">1.7.0</td>
</tr>
</tbody>
</table>
<blockquote>
<p>选择低版本 pytorch+cuda 原因：在安装 spconv 包时，由于 <code>pytorch&gt;=1.11</code> 后会删除 THC.h 文件，而 spconv 包的安装又需要该文件，若采用高版本的 pytorch 会导致安装失败。同时鉴于实验室的显卡 <code>GTX 3090</code> 仅支持 <code>CUDA &gt;= 11.0</code>，因此选择 <code>torch==1.7.0+cu110</code> 的版本</p>
</blockquote>
<h3><a id="3-2-package-installation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2 Package Installation</h3>
<h4><a id="3-2-1-spconv-installation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.1 SPCONV Installation</h4>
<p>根据<a href="https://blog.csdn.net/weixin_44808890/article/details/125387862" title="wen_1">web_1</a>，在 Conda 环境中安装相关包如下：</p>
<table>
<thead>
<tr>
<th style="text-align: center">Name</th>
<th style="text-align: center">Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><a href="https://github.com/traveller59/spconv" title="spconv install">spconv</a></td>
<td style="text-align: center">2.3.6</td>
</tr>
<tr>
<td style="text-align: center">pcdet</td>
<td style="text-align: center">0.3.0</td>
</tr>
</tbody>
</table>
<p>其中，使用 <code>pip install spconv-cu113</code> 安装 spconv 时下载的相关的包如下：</p>
<pre><code class="language-shell">ccimport-0.4.2 
certifi-2024.2.2 
charset-normalizer-3.3.2 
cumm-cu113-0.4.11 
fire-0.6.0 
idna-3.7 
lark-1.1.9 
ninja-1.11.1.1 
pccm-0.4.11 
portalocker-2.8.2 
pybind11-2.12.0 
requests-2.31.0 
six-1.16.0 
spconv-cu113-2.3.6 
termcolor-2.4.0 
urllib3-2.2.1
</code></pre>
<h4><a id="3-2-2-source-code-requirements-installation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.2 Source Code Requirements Installation</h4>
<p>下载<a href="https://github.com/hlsheng1/CT3D" title="github: CT3D">CT3D 源码</a>，将 <code>CT3D-master</code> 改名为 <code>CT3D</code> 放到 <code>home/workspace/nanyang/</code> 路径下，即代码路径为 <code>home/workspace/nanyang/CT3D</code>。</p>
<p>安装依赖项：<code>pip install -r requirements.txt</code>，下载的相关包如下：</p>
<pre><code class="language-shell">PyWavelets-1.4.1 
easydict-1.13 
imageio-2.34.0 
importlib-metadata-7.1.0 
lazy_loader-0.4 
llvmlite-0.41.1 
networkx-3.1 
numba-0.58.1 
packaging-24.0 
protobuf-5.26.1 
pyyaml-6.0.1 
scikit-image-0.21.0 
scipy-1.10.1 
tensorboardX-2.6.2.2 
tifffile-2023.7.10 
tqdm-4.66.2 
zipp-3.18.1
</code></pre>
<p>同时，利用 <code>conda install -c anaconda cmake</code> 安装 Cmake 库。</p>
<h4><a id="3-2-3-pcdet-installation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2.3 PCDet Installation</h4>
<p>通过 <code>python setup.py develop</code> 编译库。</p>
<h3><a id="3-3-dataset-preparation" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3 Dataset Preparation</h3>
<p>通过 <a href="https://blog.csdn.net/u013086672/article/details/103910266" title="web_2">web_2</a> 下载 Kitti 数据集，并通过 <a href="https://drive.google.com/file/d/1d5mq0RXRnvHPVeKx6Q612z0YRO1t2wAp/view" title="web_3">web_3</a> 下载补充的 planes 数据集，并将其转化成如下形式。</p>
<pre><code class="language-shell"># Download KITTI &amp; Planes and organize them into the following form:
├── data
│   ├── kitti
│   │   │── ImageSets
│   │   │── training
│   │   │   ├──calib &amp; velodyne &amp; label_2 &amp; image_2 &amp; (optional: planes)
│   │   │── testing
│   │   │   ├──calib &amp; velodyne &amp; image_2
</code></pre>
<p>需要注意的是 kitti 数据集中的 ImageSets 是在 OpenPCDet 的 <a href="https://github.com/open-mmlab/OpenPCDet" title="OpenPCDet source code">源码</a> 中的以下路径存放：data/kitti/ImageSets。此外，由于在服务器上运行，因此数据集无法和代码放到一起。</p>
<table>
<thead>
<tr>
<th style="text-align: center">Name</th>
<th style="text-align: left">Path</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center">Dataset</td>
<td style="text-align: left">home/share/nanyang/Kitti</td>
</tr>
<tr>
<td style="text-align: center">Code</td>
<td style="text-align: left">home/workspace/nanyang/CT3D</td>
</tr>
</tbody>
</table>
<p>这将为后面训练时会需要对代码进行一些修改。</p>
<h3><a id="3-4-generate-data-information" class="anchor" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4 Generate Data Information</h3>
<p>在路径为 <code>home/workspace/nanyang/CT3D</code> 下，采用如下的代码产生数据信息：</p>
<pre><code class="language-shell">python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml
</code></pre>
<p>首先要对 <code>home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py</code> 中数据集的路径进行修改，将其 433-439 行修改为：</p>
<pre><code class="language-shell"># 原始代码
ROOT_DIR = (Path(__file__).resolve().parent / '../../../').resolve()
create_kitti_infos(
    dataset_cfg=dataset_cfg,
    class_names=['Car', 'Pedestrian', 'Cyclist'],
    data_path=ROOT_DIR / 'data' / 'kitti',
    save_path=ROOT_DIR / 'data' / 'kitti'
)

# 修改后代码
# 此时路径为 home/share/nanyang/Kitti  
ROOT_DIR = (Path(__file__).resolve().parent / '../../../../../../share/nanyang/Kitti').resolve()
create_kitti_infos(
    dataset_cfg=dataset_cfg,
    class_names=['Car', 'Pedestrian', 'Cyclist'],
    data_path=ROOT_DIR,
    save_path=ROOT_DIR
)
</code></pre>
<p>在修改完地址后，运行会出现以下报错：</p>
<pre><code class="language-shell">/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py:127: RuntimeW
arning: 'pcdet.datasets.kitti.kitti
_dataset'found in sys.modules after import of package'podet datasets.kitti', but prior to execution of'pcdet.datasets.kitt i.kitti_dataset'; this may result in unpredictable behaviour warn (RuntimeWarning(msg))
Traceback (most recent call last):
File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py&quot;, lin e 194, in _run_module_as_main
return _run_code(code, main_globals, None,
File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy-py&quot;, lin e 87, in _run_code
exec (code, run_globals)
File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 432, in ‹module &gt;
dataset_cfg = EasyDict(yaml.load(open(sys.argv[2])))
TypeError: load() missing 1 required positional argument: 'Loader'
</code></pre>
<p>在 <a href="https://blog.csdn.net/qq_44824148/article/details/122337056" title="web_4">web_4</a> 上得知，yaml 在 5.1 版本后便不再使用 <code>yaml.load(file)</code>，观察到我们的 Conda 环境中 <code>PyYAML==6.0.1</code>，因此需要修改部分代码。</p>
<p>找到 <code>/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py</code>   , 将其 432 行的代码修改为：</p>
<pre><code class="language-shell">dataset_cfg = EasyDict(yaml.load(open(sys.argv[2])))    # 原始代码

dataset_cfg = EasyDict(yaml.load(open(sys.argv[2]), Loader = yaml.FullLoader))  # 修改后代码
</code></pre>
<p>但是会产生新的报错如下。主要是在 <code>spconv.utils</code> 中未能找到 <code>VoxelGenerator</code> 或 <code>VoxelGeneratorV2</code></p>
<pre><code class="language-shell">/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py:127: RuntimeWarning: 'pcdet.datasets.kitti.kitti_dataset' found in sys.modules after import of package 'pcdet.datasets.kitti', but prior to execution of 'pcdet.datasets.kitti.kitti_dataset'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Traceback (most recent call last):
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py&quot;, line 46, in transform_points_to_voxels
    from spconv.utils import VoxelGeneratorV2 as VoxelGenerator
ImportError: cannot import name 'VoxelGeneratorV2' from 'spconv.utils' (/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/site-packages/spconv/utils/__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 434, in &lt;module&gt;
    create_kitti_infos(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 387, in create_kitti_infos
    dataset = KittiDataset(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 22, in __init__
    super().__init__(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/dataset.py&quot;, line 33, in __init__
    self.data_processor = DataProcessor(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py&quot;, line 16, in __init__
    cur_processor = getattr(self, cur_cfg.NAME)(config=cur_cfg)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py&quot;, line 48, in transform_points_to_voxels
    from spconv.utils import VoxelGenerator
ImportError: cannot import name 'VoxelGenerator' from 'spconv.utils' (/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/site-packages/spconv/utils/__init__.py)
</code></pre>
<p>寻找原因：在 <a href="https://github.com/traveller59/spconv/issues/452" title="spconv issue">web_5</a> 中找到。将 <code>home/workspace/nanyang/CT3D</code> 中所有出现的以下代码均作修改：</p>
<pre><code class="language-shell"># 原始代码
import spconv

from spconv.utils import VoxelGenerator
(from spconv.utils import VoxelGeneratorV2 as VoxelGenerator)

# 修改后代码
import spconv.pytorch as spconv

from spconv.utils import Point2VoxelGPU3d as VoxelGenerator
</code></pre>
<p>此时报错如下：</p>
<pre><code class="language-shell">/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py:127: RuntimeWarning: 'pcdet.datasets.kitti.kitti_dataset' found in sys.modules after import of package 'pcdet.datasets.kitti', but prior to execution of 'pcdet.datasets.kitti.kitti_dataset'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
Traceback (most recent call last):
  File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 434, in &lt;module&gt;
    create_kitti_infos(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 387, in create_kitti_infos
    dataset = KittiDataset(dataset_cfg=dataset_cfg, class_names=class_names, root_path=data_path, training=False)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/kitti/kitti_dataset.py&quot;, line 22, in __init__
    super().__init__(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/dataset.py&quot;, line 33, in __init__
    self.data_processor = DataProcessor(
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py&quot;, line 16, in __init__
    cur_processor = getattr(self, cur_cfg.NAME)(config=cur_cfg)
  File &quot;/home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py&quot;, line 47, in transform_points_to_voxels
    voxel_generator = VoxelGenerator(TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:
    1. spconv.core_cc.csrc.sparse.all.ops3d.Point2Voxel(vsize_xyz: List[float[3]], coors_range_xyz: List[float[6]], num_point_features: int, max_num_voxels: int, max_num_points_per_voxel: int)Invoked with: kwargs: voxel_size=[0.05, 0.05, 0.1], point_cloud_range=array([0. , -40. ,  -3. ,  70.4,  40. ,   1.], dtype=float32), max_num_points=5, max_voxels=40000
</code></pre>
<p>可以看到是由于使用 2.x 版本的 spconv 后，其丢弃了 1.x 版本中的 VoxelGenerator 函数，变为了 Point2VoxelGPU3d 函数，其参数名称、数据类型和参数数量也会相应地改变。具体和之前变化如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>VoxelGenerator(before)</th>
<th>Point2Voxel(after)</th>
</tr>
</thead>
<tbody>
<tr>
<td>param</td>
<td>voxel_size(list)</td>
<td>vsize_xyz(list)</td>
</tr>
<tr>
<td></td>
<td>point_cloud_range(array)</td>
<td>coors_range_xyz(list)</td>
</tr>
<tr>
<td></td>
<td></td>
<td>num_point_features(int)</td>
</tr>
<tr>
<td></td>
<td>max_num_points(int)</td>
<td>max_num_points_per_voxel(int)</td>
</tr>
<tr>
<td></td>
<td>max_voxels(int)</td>
<td>max_num_voxels(int)</td>
</tr>
<tr>
<td>param.cnt</td>
<td>4</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>因此我们需要将其变量名称、变量类型、标亮数量均进行修改。注意我们在 <code>home/workspace/nanyang/CT3D/tools/cfgs/dataset_configs/kitti_dataset.yaml</code> 中找到 <code>NUM_POINT_FEATURES: 4</code> 并将其添加到 <code>DATA_PROCESSOR</code> 里面的 <code>- NAME: transform_points_to_voxels</code> 中，并在代码里进行调用。找到 <code>home/workspace/nanyang/CT3D/pcdet/datasets/processor/data_processor.py</code> 中的 47-52 行，将其修改如下：</p>
<pre><code class="language-shell"># 原始代码
voxel_generator = VoxelGenerator(
    voxel_size=config.VOXEL_SIZE,
    point_cloud_range=self.point_cloud_range,
    max_num_points=config.MAX_POINTS_PER_VOXEL,   
    max_voxels=config.MAX_NUMBER_OF_VOXELS[self.mode]
)

# 修改后代码
voxel_generator = VoxelGenerator(
    vsize_xyz=config.VOXEL_SIZE,
    coors_range_xyz=self.point_cloud_range.tolist(),
    num_point_features=config.NUM_POINT_FEATURES
    max_num_points_per_voxel=config.MAX_POINTS_PER_VOXEL,   
    max_num_voxels=config.MAX_NUMBER_OF_VOXELS[self.mode]
)
</code></pre>
<p>此时在运行代码<code>python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml</code>，发现运行成功！！！</p>
<pre><code class="language-shell">(CT3D) nanyang@vclab-gpuserver-57:/home/workspace/nanyang/CT3D$ python -m pcdet.datasets.kitti.kitti_dataset create_kitti_infos tools/cfgs/dataset_configs/kitti_dataset.yaml
/home/workspace/nanyang/anaconda3/envs/CT3D/lib/python3.8/runpy.py:127: RuntimeWarning: 'pcdet.datasets.kitti.kitti_dataset' found in sys.modules after import of package 'pcdet.datasets.kitti', but prior to execution of 'pcdet.datasets.kitti.kitti_dataset'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))
---------------Start to generate data infos---------------
train sample_idx: 000000
train sample_idx: 000003
train sample_idx: 000007
train sample_idx: 000009
train sample_idx: 000010
train sample_idx: 000011
train sample_idx: 000012
···
</code></pre>

                  </article>
                  <div class="comments-wrap">
                    <div class="share-comments">
                      

                      

                      
                    </div>
                  </div><!-- end comments wrap -->
              </div>
            </div><!-- end columns -->
      </div><!-- end container -->
    </section>



    <footer class="footer">
        <div class="content has-text-centered">
          <p>
              Copyright &copy; 2019
              Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
              Theme used <a target="_blank" href="https://bulma.io/">Bulma CSS</a>.
          </p>
        </div>
      </footer>

<style>.mweb-charts{background:#fff;}
body{ box-sizing: border-box;
    margin: 0 auto;}
@media print{
    pre, code, pre code {
     overflow: visible !important;
     white-space: pre-wrap !important;       /* css-3 */
     white-space: -moz-pre-wrap !important;  /* Mozilla, since 1999 */
     white-space: -pre-wrap !important;      /* Opera 4-6 */
     white-space: -o-pre-wrap !important;    /* Opera 7 */
     word-wrap: break-word !important;       /* Internet Explorer 5.5+ */
    }
    html,body{margin:0;padding:4px;}
}



div.code-toolbar {
  position: relative;
}

div.code-toolbar > .toolbar {
  position: absolute;
  z-index: 10;
  top: .3em;
  right: .2em;
  transition: opacity 0.3s ease-in-out;
  opacity: 0;
}

div.code-toolbar:hover > .toolbar {
  opacity: 1;
}

/* Separate line b/c rules are thrown out if selector is invalid.
   IE11 and old Edge versions don't support :focus-within. */
div.code-toolbar:focus-within > .toolbar {
  opacity: 1;
}

div.code-toolbar > .toolbar > .toolbar-item {
  display: inline-block;
}

div.code-toolbar > .toolbar > .toolbar-item > a {
  cursor: pointer;
}

div.code-toolbar > .toolbar > .toolbar-item > button {
  background: none;
  border: 0;
  color: inherit;
  font: inherit;
  line-height: normal;
  overflow: visible;
  padding: 0;
  -webkit-user-select: none; /* for button */
  -moz-user-select: none;
  -ms-user-select: none;
}

div.code-toolbar > .toolbar > .toolbar-item > a,
div.code-toolbar > .toolbar > .toolbar-item > button,
div.code-toolbar > .toolbar > .toolbar-item > span {
  color: inherit;
  font-size: .8em;
  padding: 4px .5em;
  background: #f5f2f0;
  background: rgba(224, 224, 224, 0.4);
  box-shadow: 0 2px 0 0 rgba(0,0,0,0.2);
  border-radius: .5em;
}

div.code-toolbar > .toolbar > .toolbar-item > a:hover,
div.code-toolbar > .toolbar > .toolbar-item > a:focus,
div.code-toolbar > .toolbar > .toolbar-item > button:hover,
div.code-toolbar > .toolbar > .toolbar-item > button:focus,
div.code-toolbar > .toolbar > .toolbar-item > span:hover,
div.code-toolbar > .toolbar > .toolbar-item > span:focus {
  color: inherit;
  text-decoration: none;
}
</style><script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script><script>!function(){if("undefined"!=typeof Prism&&"undefined"!=typeof document){var e=[],t={},n=function(){};Prism.plugins.toolbar={};var a=Prism.plugins.toolbar.registerButton=function(n,a){var r;r="function"==typeof a?a:function(e){var t;return"function"==typeof a.onClick?((t=document.createElement("button")).type="button",t.addEventListener("click",(function(){a.onClick.call(this,e)}))):"string"==typeof a.url?(t=document.createElement("a")).href=a.url:t=document.createElement("span"),a.className&&t.classList.add(a.className),t.textContent=a.text,t},n in t?console.warn('There is a button with the key "'+n+'" registered already.'):e.push(t[n]=r)},r=Prism.plugins.toolbar.hook=function(a){var r=a.element.parentNode;var l=a.element.classList;if(l.contains('language-mermaid') || l.contains('language-echarts') || l.contains('language-plantuml')){return;} if(r&&/pre/i.test(r.nodeName)&&!r.parentNode.classList.contains("code-toolbar")){var o=document.createElement("div");o.classList.add("code-toolbar"),r.parentNode.insertBefore(o,r),o.appendChild(r);var i=document.createElement("div");i.classList.add("toolbar");var l=e,d=function(e){for(;e;){var t=e.getAttribute("data-toolbar-order");if(null!=t)return(t=t.trim()).length?t.split(/\s*,\s*/g):[];e=e.parentElement}}(a.element);d&&(l=d.map((function(e){return t[e]||n}))),l.forEach((function(e){var t=e(a);if(t){var n=document.createElement("div");n.classList.add("toolbar-item"),n.appendChild(t),i.appendChild(n)}})),o.appendChild(i)}};a("label",(function(e){var t=e.element.parentNode;if(t&&/pre/i.test(t.nodeName)&&t.hasAttribute("data-label")){var n,a,r=t.getAttribute("data-label");try{a=document.querySelector("template#"+r)}catch(e){}return a?n=a.content:(t.hasAttribute("data-url")?(n=document.createElement("a")).href=t.getAttribute("data-url"):n=document.createElement("span"),n.textContent=r),n}})),Prism.hooks.add("complete",r)}}();</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.css"><script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script><style>div.code-toolbar > .toolbar > .toolbar-item > a, div.code-toolbar > .toolbar > .toolbar-item > button, div.code-toolbar > .toolbar > .toolbar-item > span {padding: 4px .5em; background: #f5f2f0; background: rgba(224, 224, 224, 0.4);}</style><script>window.MathJax = {     tex: { packages: {'[+]': ['physics']}, tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] },loader: { load: ['[tex]/physics'] } ,     startup: {     pageReady() {       return MathJax.startup.defaultPageReady().then(function () {          window.mweb_mathjax_ready_val = 'yes';          if(window.mweb_mathjax_ready !== undefined){ mweb_mathjax_ready(); }       });     }   }};document.addEventListener('DOMContentLoaded', function(event) {    if (typeof Prism != 'undefined') {         Prism.highlightAll();     }});window.mweb_mathjax_ready_val = '';function theMWebMathJaxRenderIsReady(key){ return window.mweb_mathjax_ready_val; }</script><script>window.MathJax = { tex: { packages: {'[+]': ['physics']}, tags: 'all', inlineMath: [ ['$','$'], ['\\(','\\)'] ] },loader: { load: ['[tex]/physics'] } }; </script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>


  
    




  </body>
</html>
